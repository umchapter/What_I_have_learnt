{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결정트리와 앙상블\n",
    "* 결정 트리는 매우 쉽고 유연하게 적용할 수 있는 알고리즘.   \n",
    "데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음.     \n",
    "예측 성능 향상을 위해 복잡한 규칙 구조를 가져야 함.     \n",
    "이로 인해 과적합(overfitting)이 발생해 반대로 예측 성능이 저하될 수 있다는 단점\n",
    "\n",
    "* 그러나 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용.      \n",
    "앙상블은 매우 많은 여러개의 약한 학습기(예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트 하면서 예측 성능을 향상시킴.     \n",
    "결정트리가 좋은 약한 학습기가 됨(GBM, XGBoost, LightGBM 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "* 결정 트리 알고리즘은 학습을 통해 데이터에 있는 규칙을 자동으로 찾아내어 트리(Tree) 기반의 분류 규칙을 만듦(If-Else 기반 규칙).\n",
    "* 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함.\n",
    "* 루트 노드 -(분할)-    \n",
    "규칙노드(규칙조건, 브랜치/ 서브트리 : 새로운 규칙 노드 기반의 서브 트리 생성) - (분할) -     \n",
    "                         리프노드(결정된 분류값)     \n",
    "                         규칙노드 - (분할) - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정보 균일도 측정 방법\n",
    "#### 정보 이득(Inforamtion Gain)\n",
    "* 정보 이득은 엔트로피라는 개념을 기반으로 함.   \n",
    "엔트로피는 주어진 데이터 집합의 혼잡도를 의미함.    \n",
    "서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮음.      \n",
    "정보 이득 지수는 1에서 엔트로피 지수를 뺀 값. 즉, 1-엔트로피 지수.      \n",
    "결정 트리는 이 정보 이득 지수로 분할 기준을 정함. 즉, 정보 이득이 높은 속성을 기준으로 분할함.\n",
    "&nbsp;\n",
    "#### 지니 계수\n",
    "* 지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수.    \n",
    "0이 가장 평등, 1로 갈수록 불평등.   \n",
    "머신러닝에 적용될 때는 의미론 적으로 재해석돼 데이터가 다양한 값을 가질수록 평등, 특정 값으로 쏠릴 경우에는 불평등한 값.    \n",
    "즉, 다양성이 낮을 수록 균일도가 높다는 의미, 1로 갈수록 균일도가 높으므로 지니 계수가 높은 속성을 기준으로 분할하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree 분할 규칙\n",
    "* 기본적으로 지니 계수를 이용해 데이터 세트를 분할.\n",
    "* 정보이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식트리에 노드를 분할.\n",
    "* 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류 결정.\n",
    "#### Decision Tree의 특징\n",
    "* \"균일도\" 직관적이고 쉽다.\n",
    "* 트리의 크기를 사전에 제한하는 것이 성능 튜닝에 효과적.    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 모든 데이터를 만족하는 완벽한 규칙은 만들 수 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth\n",
    "* 트리의 최대 깊이를 규정.\n",
    "* 디폴트는 None. None으로 설정하면 min_sample_split까지 완전 분할, overfitting 우려.\n",
    "##### max_features\n",
    "* 최적 분할을 위해 고려할 최대 피처 개수. 디폴트 None, 모든 피처 사용하여 분할.\n",
    "* int 형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트.\n",
    "* 'sqrt'는 전체 피처 중 √(전체 피처) 개수 만큼 선정\n",
    "* 'auto'로 지정하면 sqrt와 동일\n",
    "* 'log'는 전체 피처 중 log2(전체 피처 개수)선정\n",
    "* 'None'은 전체 피처 선정\n",
    "##### min_samples_split\n",
    "* 노드를 분할하기 위한 최소한의 샘플 데이터수로 과적합을 제어하는 데 사용.\n",
    "* 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
    "##### min_samples_leaf\n",
    "* 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수\n",
    "* min_samples_split와 유사하게 과적합 제어 용도. 그러나 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요.\n",
    "##### max_leaf_nodes\n",
    "* 말단 노드(Leaf)의 최대 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DecisionTreeClassifier 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=121)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\n",
    "                                                    test_size=0.2, random_state=11)\n",
    "\n",
    "# DecisionTreeCalssifier 학습.\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names,\n",
    "                feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "# impurity - 불순도를 gini로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz로 읽어서 code 상에서 시각화\n",
    "with open(\"tree.dot\") as f :\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth=3 짜리 트리  /  보통 홀수 깊이 설정\n",
    "with open(\"tree2.dot\") as f :\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# feature importance 추출\n",
    "print(f\"Feature Importance : {np.round(dt_clf.feature_importances_, 3)}\")\n",
    "\n",
    "# feature 별 importance 매핑\n",
    "for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_) :\n",
    "    print(f\"{name} : {value:.3f}\")\n",
    "\n",
    "# feature importance를 column별로 시각화 하기\n",
    "sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결정트리 과적합(Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"3 Class values with 2 Features Sample data creation\")\n",
    "\n",
    "# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의\n",
    "# classification 샘플 데이터 생성\n",
    "X_features, y_labels = make_classification(n_features=2, n_redundant=0,\n",
    "                                          n_informative=2, n_classes=3,\n",
    "                                          n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# plot 형태로 2개의 features로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.\n",
    "plt.scatter(X_features[:,0], X_features[:,1], marker=\"o\", c=y_labels, s=25,\n",
    "            cmap=\"rainbow\", edgecolors=\"k\")                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Classifier의 Decision Boundary를 시각화 하는 함수\n",
    "def visualize_boundary(model, X, y) :\n",
    "    # 서브플롯들의 형태와 개별 플롯\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # 학습 데이터 scatter plot으로 나타내기\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=\"rainbow\", edgecolors=\"k\", \n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    xlim_start, xlim_end = ax.get_xlim()\n",
    "    ylim_start, ylim_end = ax.get_ylim()\n",
    "\n",
    "    # 호출 파라미터로 들어온 training 데이터로 model 학습\n",
    "    model.fit(X,y)\n",
    "    # meshgrid 형태인 모든 좌표값으로 예측 수행\n",
    "    # meshgrid는 격자모양의 좌표평면\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim_start, xlim_end, num=200),\n",
    "                         np.linspace(ylim_start, ylim_end, num=200))\n",
    "    # np.c_는 2차원축을 기준으로 병합 → 평탄화 된 xx와 yy 결합\n",
    "    # ndarray.ravel는 평탄화\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # contourf()를 이용하여 class boundary를 visualization 수행\n",
    "    n_classes = len(np.unique(y))\n",
    "    # levels는 컨투어 라인의 갯수를 나타내는 array, 증가형태여야 함 또는 int\n",
    "    # X,y는 Z에 들어가는 좌표값들\n",
    "    # Z는 컨투어가 그려지는 높이 값들\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                          levels=np.arange(n_classes + 1) - 0.5,\n",
    "                          cmap=\"rainbow\", clim=(y.min(), y.max()),\n",
    "                          zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 특정한 트리 생성 제약없는 결정트리의 Decision Boundary 시각화\n",
    "dt_clf_bdry = DecisionTreeClassifier()\n",
    "visualize_boundary(dt_clf_bdry, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정한 트리 생성 제약없는 결정트리의 Decision Boundary 시각화\n",
    "dt_clf_bdry_sbj = DecisionTreeClassifier(min_samples_leaf=6)\n",
    "visualize_boundary(dt_clf_bdry_sbj, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "* 앙상블의 유형은 일반적으로 Voting, Bagging, Boosting 으로 구분, 이외에 Stacking 등의 기법이 있음\n",
    "* 대표적인 배깅은 Random Forest 알고리즘이 있으며, 부스팅에는 에이다 부스팅, 그래디언트 부스팅, XGBoost, LightGBM 등이 있음.    \n",
    "배깅은 bootstrap + aggregating  /  데이터 샘플 복원추출 + 집계       \n",
    "정형 데이터의 분류나 회귀에서는 GBM 부스팅 계열의 앙상블이 전반적으로 높은 예측 성능을 나타냄.\n",
    "* 넓은 의미로는 서로 다른 모델을 결합한 것들을 앙상블로 지칭하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 앙상블의 특징\n",
    "* 단일 모델의 약점을 다수의 모델들을 결합하여 보완\n",
    "* 뛰어난 성능을 가진 모델들로만 구성하는 것보다 성능이 떨어지더라도 서로 다른 유형의 모델을 섞는 것이 오히려 전체 성능에 도움이 될 수 있음\n",
    "* 랜덤 포레스트 및 뛰어난 부스팅 알고리즘들은 모두 결정 트리 알고리즘을 기반 알고리즘으로 적용함\n",
    "* 결정 트리의 단점인 overfitting을 수십~수천개의 많은 분류기를 결합해 보완하고 장점인 직관적인 분류 기준은 강화됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting Type\n",
    "* Hard Voting은 단순 다수결\n",
    "* Soft Voting은 클래스의 확률로 결과를 보정\n",
    "* 일반적으로 HV 보다는 SV이 예측 성능이 상대적으로 우수하여 주로 사용됨\n",
    "* 사이킷런은 VotingClassifier 클래스를 통해 Voting을 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 모델은 로지스틱 회귀와 KNN\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# 개별 모델을 Soft Voting 기반의 앙상블 모델로 구현한 분류기\n",
    "vo_clf = VotingClassifier(estimators=[(\"LR\",lr_clf), (\"KNN\", knn_clf)],\n",
    "                          voting=\"soft\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    test_size=0.2, random_state=156)\n",
    "\n",
    "# VotingClassifier 적용\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print(f\"Voting 분류기 정확도 : {accuracy_score(y_test, pred): .4f}\")\n",
    "\n",
    "# 개별 모델의 적용\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers :\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print(f\"{class_name} 정확도 : {accuracy_score(y_test, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Prac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DF로 로드.\n",
    "featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "featuer_name = featuer_name_df.iloc[:,1].values.tolist()\n",
    "print(f\"전체 피처명에서 10개만 추출 : {featuer_name[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dup_df = featuer_name_df.groupby(\"column_name\").count()\n",
    "feature_dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dup_df = featuer_name_df.groupby(\"column_name\").count()\n",
    "print(feature_dup_df[feature_dup_df[\"column_index\"]>1].count())\n",
    "feature_dup_df[feature_dup_df[\"column_index\"]>1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 feature명에 대해서는 뒤에 번호 붙이는 함수\n",
    "def get_new_feature_name_df(old_feature_name_df) :\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(\"column_name\").cumcount(),\n",
    "                                  columns=[\"dup_cnt\"])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=\"outer\")\n",
    "    new_feature_name_df[\"column_name\"] = new_feature_name_df[[\"column_name\", \"dup_cnt\"]].apply(lambda x : x[0]+\"_\"+str(x[1])\n",
    "                                                                                        if x[1]>0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop([\"index\"], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_dataset() :\n",
    "\n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df() 를 이용, 신규 피처명 DF 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(featuer_name_df)\n",
    "\n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featuer_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    # 학습 피처 데이터 셋과 테스스 피처 데이터를 DF로 로딩, 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "    X_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "\n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DF로 로딩하고 컬럼명은 action으로 부여\n",
    "    # \"\\s+\" 데이터 사이 간격 공백으로 구분\n",
    "    y_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "    y_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "\n",
    "    # 로드된 학습/테스트용 DF를 모두 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## 학습 피처 데이터셋 info()\")\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[\"action\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum().sum()  # Null값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"결정 트리 예측 정확도 : {accuracy:.4f}\")\n",
    "\n",
    "# DecisionTreeClassifier의 하이퍼 파라미터 추출\n",
    "print(f\"DecisionTreeClassifier 기본 하이퍼 파라미터 : \\n {dt_clf.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"max_depth\" : [6,8,10,12,16,20,24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=\"accuracy\", cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_}\")\n",
    "print(f\"GridSearchCV 최적 하이퍼 파라미터 : {grid_cv.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV 객체의 cv_results_ 속성을 DataFrame으로 생성\n",
    "cv_results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "\n",
    "# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\n",
    "cv_results_df[[\"param_max_depth\", \"mean_test_score\"]]\n",
    "\n",
    "# 최적 깊이 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [6,8,10,12,16,20,24]\n",
    "# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정\n",
    "for depth in max_depths :\n",
    "    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print(f\"max_depth = {depth} 정확도 {accuracy:.4f}\")\n",
    "\n",
    "# 최고 정확도 깊이 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\" : [8, 12, 16,20],\n",
    "    \"min_samples_split\" : [16, 24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=\"accuracy\", cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_}\")\n",
    "print(f\"GridSearchCV 최적 하이퍼 파라미터 : {grid_cv.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df_clf = grid_cv.best_estimator_\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred1)\n",
    "print(f\"결정 트리 예측 정확도 {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ftr_importance_values = best_df_clf.feature_importances_\n",
    "# Top 중요도로 정렬을 쉽게 하고, 시본의 막대그래프로 쉽게 표현하기 위해 Series변환\n",
    "ftr_importance = pd.Series(ftr_importance_values, index=X_train.columns)\n",
    "# 중요도값 순으로 Series를 정렬\n",
    "ftr_top20 = ftr_importance.sort_values(ascending=False)[:20]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Feature Importance Top 20\")\n",
    "sns.barplot(x=ftr_top20, y=ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_feature_name_df(old_feature_name_df) :\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(\"column_name\").cumcount(),\n",
    "                                  columns=[\"dup_cnt\"])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=\"outer\")\n",
    "    new_feature_name_df[\"column_name\"] = new_feature_name_df[[\"column_name\", \"dup_cnt\"]].apply(lambda x : x[0]+\"_\"+str(x[1])\n",
    "                                                                                        if x[1]>0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop([\"index\"], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_dataset() :\n",
    "\n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df() 를 이용, 신규 피처명 DF 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(featuer_name_df)\n",
    "\n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featuer_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    # 학습 피처 데이터 셋과 테스스 피처 데이터를 DF로 로딩, 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "    X_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "\n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DF로 로딩하고 컬럼명은 action으로 부여\n",
    "    # \"\\s+\" 데이터 사이 간격 공백으로 구분\n",
    "    y_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "    y_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "\n",
    "    # 로드된 학습/테스트용 DF를 모두 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# 결정 트리에서 사용한 get_human_dataset()을 이용해 DF반환\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"랜덤 포레스트 정확도 : {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\" : [100],\n",
    "    \"max_depth\" : [6, 8, 10, 12],\n",
    "    \"min_samples_leaf\" : [8, 12, 18],\n",
    "    \"min_samples_split\" : [8, 16, 20]\n",
    "}\n",
    "# RandomForestClassifier 객체 생성 후 GridSerchCV 수행\n",
    "rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)  # pc의 모든 리소스 사용\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"최적 하이퍼 파라미터 : \\n {grid_cv.best_params_}\")\n",
    "print(f\"최고 예측 정확도 : {grid_cv.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8,\n",
    "                                 min_samples_split=8, random_state=0)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "pred = rf_clf1.predict(X_test)\n",
    "print(f\"예측 정확도 : {accuracy_score(y_test, pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importance_values = rf_clf1.feature_importances_\n",
    "ftr_importance = pd.Series(ftr_importance_values, index=X_train.columns)\n",
    "ftr_top20 = ftr_importance.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Feature Importance Top 20\")\n",
    "sns.barplot(x=ftr_top20, y = ftr_top20.index)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d240ba0dc525c389faa33f5dcce5b4f32b6d6aa6d70d6d2dd929bd2b09ab69f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
