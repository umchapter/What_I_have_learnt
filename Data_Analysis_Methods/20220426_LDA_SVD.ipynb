{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA(Linear Discriminant Analysis)\n",
    "===\n",
    "* LDA는 선형 판별 분석법으로 불리며, PCA와 매우 유사함.\n",
    "* LDA는 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법.   \n",
    "  중요한 차이는 LDA는 지도학습의 분류(Classification)에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소함.   \n",
    "  PCA는 입력 데이터의 변동성이 가장 큰 축을 찾지만, LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾음\n",
    "* LDA는 같은 클래스의 데이터는 최대한 근접해서, 다른 클래스의 데이터는 최대한 떨어뜨리는 축을 매핑함.\n",
    "<center>\n",
    "<img src=\"C:/Users/user/Desktop/Vocational_Training/FinTech/images/LDA.png\">\n",
    "</center>\n",
    "\n",
    "## LDA 차원 축소 방식\n",
    "* LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산(between-class scatter)과 클래스 내부 분산(within-calss scatter)의 비율을 최대화하는 방식으로 차원을 축소함.\n",
    "* 즉, 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산은 최대한 작게 가져가는 방식.\n",
    "<center>\n",
    "<img src=\"C:/Users/user/Desktop/Vocational_Training/FinTech/images/LDA_variance.png\">\n",
    "</center>\n",
    "\n",
    "## LDA 절차\n",
    "* 일반적으로 LDA를 구하는 스탭은 PCA와 유사하나, 가장 큰 차이점은 공분산 행렬이 아니라 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영한다는 점임.\n",
    "1. 클래스 내부와 클래스 간 분산 행렬을 구함.   \n",
    "  이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 벡터(mean vector)를 기반으로 구함.\n",
    "2. 클래스 내부 분산 행렬을 $S_W$, 클래스 간 분산 행렬을 $S_B$라고 하면 다음 식으로 두 행렬을 고유벡터로 분해할 수 있음.\n",
    "$$S_W^T S_B = \\begin{bmatrix}e_1&\\cdots&e_n\\\\ \\end{bmatrix}\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_n \\end{bmatrix}\\begin{bmatrix}e_1\\\\\\vdots\\\\{e_n} \\end{bmatrix}$$\n",
    "3. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출함.\n",
    "4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환함.\n",
    "\n",
    "### 실습 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해 SVD\n",
    "===\n",
    "## 대표적인 행렬 분해 방법\n",
    "### 고유값 분해(Eigen-Decomposition)\n",
    "$$C=P{\\Lambda}P^T$$\n",
    "$$C = \\begin{bmatrix}e_1&\\cdots&e_n\\\\ \\end{bmatrix}\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_n \\end{bmatrix}\\begin{bmatrix}e_1\\\\\\vdots\\\\{e_n} \\end{bmatrix}$$\n",
    "* 정방행렬만을 고유벡터로 분해.\n",
    "* PCA는 분해된 고유벡터에 원본 데이터를 투영하여 차원 축소.\n",
    "### 특이값 분해(Singular Value Decomposition)\n",
    "$$A=U{\\Sigma}V^T$$\n",
    "* SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 $m{\\times}n$행렬도 분해 가능.\n",
    "* $U$ : 왼쪽 직교행렬, ${\\Sigma}$ : 대각 행렬, $V^T$ : 오른쪽 직교행렬\n",
    "* 행렬 $U$와 $V$에 속한 벡터는 특이벡터(singular vector)이며, 모든 특이벡터는 서로 직교하는 성질을 가짐.\n",
    "$$U^TU=I$$\n",
    "$$V^TV=I$$\n",
    "* ${\\Sigma}$는 대각행렬이며, 행렬의 대각에 위치한 값만 0이 아니고 나머지 위치의 값은 모두 0.\n",
    "* ${\\Sigma}$이 위치한 0이 아닌 값이 바로 행렬 $A$의 특이값.\n",
    "## SVD 유형\n",
    "<center>\n",
    "<img src=\"C:/Users/user/Desktop/Vocational_Training/FinTech/images/SVD_classes.png\">\n",
    "</center>\n",
    "\n",
    "### Truncated SVD 행렬 분해의 의미\n",
    "$$A_{(m{\\times}n)}{\\approx}\\begin{bmatrix}m{\\times}r\\end{bmatrix} \\begin{bmatrix}r{\\times}r\\end{bmatrix} \\begin{bmatrix}r{\\times}n\\end{bmatrix} = A'_{(m{\\times}n)}$$\n",
    "* SVD는 차원 축소를 위한 행렬 분해를 통해 Latent Factor(잠재 요인)을 찾을 수 있는데, 이렇게 찾아진 Latent Factor는 많은 분야에 활용(추천 엔진, 문서의 잠재 의미 분석 등).\n",
    "* SVD로 차원 축소 행렬 분해된 후 다시 분해된 행렬을 이용하여 원복된 데이터 셋은 잡음(Noise)이 제거된 형태로 재구성 될 수 있음.\n",
    "* 사이킷런에서는 TruncatedSVD로 차원을 축소할 때 원본 데이터에 $U{\\Sigma}$를 적용하여 차원 축소.\n",
    "## SVD 활용\n",
    "* 이미지 압축/변환\n",
    "* 추천엔진\n",
    "* 문서 잠재 의미 분석\n",
    "* 의사 역행렬을 통한 모델 예측\n",
    "### 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
