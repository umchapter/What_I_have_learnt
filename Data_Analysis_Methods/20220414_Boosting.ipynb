{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "* 부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 토앻 오류를 개선해 나가는 학습 방법\n",
    "* weak learner → 과적합 피하기 쉬움\n",
    "* 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있음\n",
    "* 그래디언트 부스팅 → 기울기 기반 오류 감소 모형   \n",
    "일차 분류의 오류에 대해서 다시 분류, 반복\n",
    "* XGB → 손실함수 지정, 오버피팅, 소요시간 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM Hyper Parameters\n",
    "* loss : 경사 하강법에서 사용할 비용 함수 지정. 특별한 값 없으면 기본값인 'deviance'를 그대로 적용\n",
    "* learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. 0~1 사이의 값.    \n",
    "너무 작으면 반복이 완료되어도 못 찾음, 반대로 너무 커도 오류 못 찾고 지나칠 수 있음.\n",
    "* n_estimators : weak learner의 개수. 개수 많으면 일정 수준까지는 좋아질 수 있음.   \n",
    "반면 개수가 많아지면 시간이 오래 걸림. 기본값 100\n",
    "* subsample : 데이터 샘플링 비율. 기본값 1, 전체 기반 학습.     \n",
    "0~1사이 설정 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from modules import HmDt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = HmDt.get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"GBM 정확도 : {gb_accuracy:.4f}\")\n",
    "print(f\"GBM 수행시간 : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 500],\n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"최적 하이퍼 파라미터 : {grid_cv.best_params_}\")\n",
    "print(f\"최고 예측 정확도 : {grid_cv.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행\n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print(f\"GBM 정확도 : {gb_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)\n",
    "Parameters\n",
    "\n",
    "-   **n_estimators**  ([_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "    \n",
    "-   **max_depth**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Maximum tree depth for base learners.\n",
    "    \n",
    "-   **learning_rate**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Boosting learning rate (xgb's \"eta\")\n",
    "    \n",
    "-   **verbosity**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    \n",
    "-   **objective**  ([_Union_](https://docs.python.org/3.6/library/typing.html#typing.Union \"(in Python v3.6)\")_[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_,_ [_Callable_](https://docs.python.org/3.6/library/typing.html#typing.Callable \"(in Python v3.6)\")_[__[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__,_ [_Tuple_](https://docs.python.org/3.6/library/typing.html#typing.Tuple \"(in Python v3.6)\")_[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__]__,_ _NoneType__]_) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "    \n",
    "-   **booster**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "    \n",
    "-   **tree_method**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It's recommended to study this option from the parameters document:  [https://xgboost.readthedocs.io/en/latest/treemethod.html](https://xgboost.readthedocs.io/en/latest/treemethod.html).\n",
    "    \n",
    "-   **n_jobs**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n",
    "    \n",
    "-   **gamma**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    \n",
    "-   **min_child_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "    \n",
    "-   **max_delta_step**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Maximum delta step we allow each tree's weight estimation to be.\n",
    "    \n",
    "-   **subsample**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of the training instance.\n",
    "    \n",
    "-   **colsample_bytree**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns when constructing each tree.\n",
    "    \n",
    "-   **colsample_bylevel**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each level.\n",
    "    \n",
    "-   **colsample_bynode**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each split.\n",
    "    \n",
    "-   **reg_alpha**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L1 regularization term on weights (xgb's alpha).\n",
    "    \n",
    "-   **reg_lambda**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L2 regularization term on weights (xgb's lambda).\n",
    "    \n",
    "-   **scale_pos_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Balancing of positive and negative weights.\n",
    "    \n",
    "-   **base_score**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – The initial prediction score of all instances, global bias.\n",
    "    \n",
    "-   **random_state**  (_Optional__[__Union__[_[_numpy.random.RandomState_](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState \"(in NumPy v1.22)\")_,_ [_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]__]_) –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, \\\n",
    "                            recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from modules import Eval\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_set = load_breast_cancer()\n",
    "brst_data = data_set.data\n",
    "brst_labels = data_set.target\n",
    "\n",
    "brst_df = pd.DataFrame(data=brst_data, columns=data_set.feature_names)\n",
    "brst_df[\"target\"] = brst_labels\n",
    "\n",
    "brst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(brst_data, brst_labels,\n",
    "                                                    test_size=0.3, random_state=121)\n",
    "\n",
    "brst_xgb_clt = xgb.XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "brst_xgb_clt.fit(X_train, y_train)\n",
    "\n",
    "brst_pred = brst_xgb_clt.predict(X_test)\n",
    "brst_pred_proba = brst_xgb_clt.predict_proba(X_test)\n",
    "\n",
    "Eval.get_clf_eval(y_test, brst_pred, brst_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "plot_importance(brst_xgb_clt, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brst_xgb_clt_es = xgb.XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "brst_xgb_clt_es.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"logloss\", eval_set=[(X_test, y_test)])\n",
    "\n",
    "brst_pred_es = brst_xgb_clt_es.predict(X_test)\n",
    "brst_pred_proba_es = brst_xgb_clt_es.predict_proba(X_test)\n",
    "\n",
    "Eval.get_clf_eval(y_test, brst_pred_es, brst_pred_proba_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "* SVM은 분류에 사용되는 지도학습 머신러닝 모델이다.\n",
    "* SVM은 서포트 벡터(support vectors)를 사용해서 결정 경계(Decision Boundary)를 정의하고, 분류되지 않은 점을 해당 결정 경계와 비교해서 분류한다.\n",
    "* 서포트 벡터(support vectors)는 결정 경계에 가장 가까운 각 클래스의 점들이다.\n",
    "* 서포트 벡터와 결정 경계 사이의 거리를 마진(margin)이라고 한다.\n",
    "* SVM은 허용 가능한 오류 범위 내에서 가능한 최대 마진을 만들려고 한다.\n",
    "* 파라미터 C는 허용되는 오류 양을 조절한다. C 값이 클수록 오류를 덜 허용하며 이를 하드 마진(hard margin)이라 부른다. 반대로 C 값이 작을수록 오류를 더 많이 허용해서 소프트 마진(soft margin)을 만든다.\n",
    "* SVM에서는 선형으로 분리할 수 없는 점들을 분류하기 위해 커널(kernel)을 사용한다.\n",
    "* 커널(kernel)은 원래 가지고 있는 데이터를 더 높은 차원의 데이터로 변환한다. 2차원의 점으로 나타낼 수 있는 데이터를 다항식(polynomial) 커널은 3차원으로, RBF 커널은 점을 무한한 차원으로 변환한다.\n",
    "* RBF 커널에는 파라미터 감마(gamma)가 있다. 감마가 너무 크면 학습 데이터에 너무 의존해서 오버피팅이 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 감마값이 작으면 언더피팅 위험  /  C 값이 크면 하드마진에 의한 오버피팅 위험\n",
    "sv_clf = SVC(gamma=0.001, C=100.)\n",
    "sv_clf.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "sv_pred = sv_clf.predict(X_iris_test)\n",
    "print(y_iris_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_iris_test, sv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (k-Nearest Neighbour)\n",
    "* 레이블 값을 미리 주는 지도학습\n",
    "* 장점 : 어떤 종류의 학습이나 준비 시간이 필요 없음\n",
    "* 단점 : 특징 공간에 있는 모든 데이터에 대한 정보가 필요함      \n",
    "가장 가까운 이웃을 찾기 위해 새로운 데이터에서 모든 기존 데이터까지의 거리를 확인해야 하기 때문     \n",
    "데이터와 클래스가 많이 있으면, 많은 메모리 공간과 계산 시간이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "sepal = iris.data[:, 0:2]\n",
    "kind = iris.target\n",
    "\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.plot(sepal[kind==0][:,0], sepal[kind==0][:,1],\n",
    "        \"ro\", label=\"Setosa\")\n",
    "plt.plot(sepal[kind==1][:,0], sepal[kind==1][:,1],\n",
    "        \"bo\", label=\"Versicolor\")\n",
    "plt.plot(sepal[kind==2][:,0], sepal[kind==2][:,1],\n",
    "        \"yo\", label=\"Verginica\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_i_knn = iris.data\n",
    "y_i_knn = iris.target\n",
    "\n",
    "X_i_knn_train, X_i_knn_test, y_i_knn_train, y_i_knn_test = train_test_split(\n",
    "    X_i_knn, y_i_knn, test_size=0.2, random_state=121\n",
    ")\n",
    "\n",
    "print(X_i_knn_train.shape)\n",
    "print(X_i_knn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "knn.fit(X_i_knn_train, y_i_knn_train)\n",
    "\n",
    "knn_pred = knn.predict(X_i_knn_test)\n",
    "print(accuracy_score(y_i_knn_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn2.fit(X_i_knn, y_i_knn)\n",
    "\n",
    "# 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "classes = {\n",
    "    0 : \"Setosa\",\n",
    "    1 : \"Versicolor\",\n",
    "    2 : \"Virginica\"\n",
    "}\n",
    "\n",
    "# 새로운 데이터 제시\n",
    "X_new = [[3,4,5,2],\n",
    "        [5,4,2,2]]\n",
    "y_predict = knn.predict(X_new)\n",
    "\n",
    "print(classes[y_predict[0]])\n",
    "print(classes[y_predict[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "* 파이프라인을 사용하면 데이터 사전 처리 및 분류의 모든 단계를 포함하는 단일 개체를 만들 수 있다.\n",
    "\n",
    "* train과 test 데이터 손실을 피할 수 있다.\n",
    "* 교차 검증 및 기타 모델 선택 유형을 쉽게 만든다.\n",
    "* 재현성 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_rand, y_rand = make_classification(n_samples=100, n_features=10, n_informative=2)\n",
    "\n",
    "X_rand_train, X_rand_test, y_rand_train, y_rand_test = train_test_split(X_rand, y_rand,\n",
    "                                                        test_size=0.3, random_state=121)\n",
    "\n",
    "X_rand_train.shape, X_rand_test.shape, y_rand_train.shape, y_rand_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a list of tuples parameter\n",
    "pipline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# use the pipeline object as you would do with a regular classifier\n",
    "pipline.fit(X_rand_train, y_rand_train)\n",
    "\n",
    "y_rand_pred = pipline.predict(X_rand_test)\n",
    "\n",
    "accuracy_score(y_rand_test, y_rand_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d240ba0dc525c389faa33f5dcce5b4f32b6d6aa6d70d6d2dd929bd2b09ab69f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
