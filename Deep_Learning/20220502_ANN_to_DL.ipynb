{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에서 딥러닝으로\n",
    "===\n",
    "## 1. 기울기 소실 문제와 활성화 함수\n",
    "### DNN과 기울기 소실 문제\n",
    "* DNN은 MLP(다층 퍼셉트론)에서 은닉층의 개수를 증가시킨 것.\n",
    "  * \"딥(deep)\"이라는 용어는 은닉층이 깊다는 것을 의미함.\n",
    "  * 최근 딥러닝은 컴퓨터 비전, 음성 인식, 자연어 처리, 소셜 네트워크 필터링, 기계 번역 등에 적용되어서 인간 전문가에 필적하는 결과를 얻고 있음.\n",
    "* 기울기 소실(gradient vanishing) 문제 : 은닉층이 많아지면 출력층에서 계산된 기울기가 역전파되다가 값이 점점 작아져서 없어짐.\n",
    "* 과잉 적합(over fitting)\n",
    "  * 퍼셉트론에서는 계단 함수(step function)를 활성화 함수로 사용했지만, MLP에서는 다양한 비선형 함수들을 활성화 함수로 사용함.\n",
    "  * Sigmoid, TanH, ReLU 등\n",
    "* 다층 퍼셉트론이 오차 역전파를 만나 신경망이 되었고, 신경망은 XOR 문제를 해결.\n",
    "* 신경망을 쌓아 올려도 사람처럼 생각하고 판단하는 인공지는이 완성되지는 않음.   \n",
    "→ 원인은 기울기 소실 문제.\n",
    "### 기울기 소실 문제를 해결하기 위한 활성화 함수의 변화\n",
    "* 가중치를 수정하려면 미분 값, 즉 기울기가 필요함. 층이 늘어나면서 기울기 값이 점점 작아져 맨 처음 층까지 전달되지 않는 기울기 소실 문제 발생.\n",
    "  * 기울기 소실 문제가 발생하기 시작한 것은 활성화 함수로 사용된 시그모이드 함수의 특성 때문.\n",
    "  * 시그모이드 함수의 특성상 아주 큰 양수나 아주 큰 음수가 들어오면 출력이 포화되어서 거의 0이 됨.\n",
    "  * 시그모이드 함수를 미분하면 최대치가 0.3이므로 1보다 작으므로 계속 곱하다 보면 0에 가까워짐.\n",
    "  * 이를 대체하기 위해 활성화 함수를 시그모이드가 아닌 다른 함수들로 대체함.\n",
    "    * tanH, ReLU, softplus 등\n",
    "        1. Hyperbolic Tangent function\n",
    "           * 미분한 값의 범위가 함께 확장되는 효과를 가져옴.\n",
    "           * 여전히 1보다 작은 값이 존재하므로 기울기 소실 문제는 사라지지 않음.\n",
    "        2. ReLU function\n",
    "           * 시그모이드 함수의 대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수임.\n",
    "           * 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음.\n",
    "           * 간단한 방법을 통해 여러 층을 쌓을 수 있게 했고, 딥러닝의 발전에 속도가 붙게 됨.\n",
    "        3. softplus function\n",
    "           * 이후 렐루의 0이 되는 순간을 완화\n",
    "## 2. 속도와 정확도 문제를 해결하는 고급 경사 하강법\n",
    "* 경사 하강법은 정확하게 가중치를 찾아가지만, 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있음.\n",
    "* 경사 하강법의 불필요하게 많은 계산량은 속도를 느리게 할 뿐 아니라, 최적 해를 찾기 전에 최적화 과정을 멈추게 할 수도 있음.\n",
    "* 이러한 점을 보완한 고급 경사 하강법이 등장하면서 딥러닝의 발전 속도는 더 빨라짐.\n",
    "### 1. 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "* 전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용함.\n",
    "* 일부 데이터를 사용하므로 더 빨리 그리고 자주 업데이트를 하는 것이 가능해짐.\n",
    "* 속도가 빠르고 최적 해에 근사한 값을 찾아낸다는 장점 덕분에 경사 하강법의 대안으로 사용됨.   \n",
    "<img src=\"C:/Users/user/Desktop/Vocational_Training/FinTech/images/SGD.png\">\n",
    "\n",
    "### 2. 모멘텀(Momentum)\n",
    "* 모멘텀이란 단어는 '관성, 탄력, 가속도'라는 뜻.\n",
    "* 모멘텀 SGD란 말 그대로 경사 하강법에 탄력을 더해 주는 것.\n",
    "* 다시 말해서, 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향$(+, -)$을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법.\n",
    "* 수정 방향이 양수$(+)$ 방향으로 한 번, 음수$(-)$ 방향으로 한 번 지그재그로 일어나는 현상이 줄어들고, 이전 이동 값을 고려하여 일정 비율만큼만 다음 값을 결정하므로 관성의 효과를 낼 수 있음.   \n",
    "<img src=\"C:/Users/user/Desktop/Vocational_Training/FinTech/images/momentum.png\">\n",
    "\n",
    "### 3. 현재 가장 많이 사용되는 고급 경사 하강법\n",
    "* 아담(ADAM) : Momentum과 RMSProp 방법을 합친 방법\n",
    "  * 정확도와 보폭 크기 개선\n",
    "  * keras.optimizers.Adam(lr, beta_1, beta_2, epsilon, decay)\n",
    "  * RMPSProp : Adagrad의 보폭 민감도를 보완한 방법\n",
    "  * Adagrad : 변수의 업데이트가 잦으면 학습률을 적게하여 이동 보폭을 조절하는 방법.\n",
    "### 4. 손실함수\n",
    "* 손실 함수로는 이제까지는 제곱 오차 함수(Mean Squared Error : MSE)\n",
    "$$\\Epsilon=\\frac{1}{2}\\displaystyle\\sum_{i=1}^n(t-o)^2$$\n",
    "* 노드의 활성화 함수로 시그모이드(sigmoid)가 사용된다면 MSE는 저속 수렴 문제(slow convergence)에 부딪치게 됨.\n",
    "  * 예를 들어서 목표값이 0.0이고 출력값이 10.0이라고 하면, 차이는 무려 10.0이나 되지만 시그모이드 함수의 그래디언트는 거의 0이 됨.\n",
    "  * 반대로 목표값이 0.0이고 출력값이 1.0이라고 하면, 차이는 1.0뿐이지만 그래디언트는 0.2 정도가 나옴.\n",
    "  * 이것은 마치 시험 성적이 안좋은 학생에게 더 좋은 학점을 주는 격.\n",
    "## 3. 코드 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0901 - accuracy: 0.2366\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.4118\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.5088\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.5965\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0665 - accuracy: 0.6598\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.7083\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0563 - accuracy: 0.7440\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.7691\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0491 - accuracy: 0.7880\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.8019\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.8132\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.8222\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.8293\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.8357\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.8403\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0363 - accuracy: 0.8442\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.8472\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.8501\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0335 - accuracy: 0.8530\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0327 - accuracy: 0.8557\n",
      "테스트 손실값 : 0.031353116035461426\n",
      "테스트 정확도 : 0.8655999898910522\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128    # 가중치를 변경하기 전에 처리하는 샘플의 개수\n",
    "num_classes = 10    # 출력 클래스의 개수\n",
    "epochs = 20         # 에포크의 개수\n",
    "\n",
    "# 데이터를 학습 데이터와 테스트 데이터로 나눔.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 입력 이미지를 2차원에서 1차원 벡터로 변경함.\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# 입력 이미지의 픽셀 값이 0.0에서 1.0 사이의 값이 되게 함.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# 클래스의 개수에 따라서 하나의 출력 픽셀만이 1이 되게 함.\n",
    "# 예들 들면 1 0 0 0 0 0 0 0 0 0 과 같음.\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 신경망의 모델을 구축함.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(512, activation=\"sigmoid\", input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.1)\n",
    "\n",
    "# 손실 함수를 제곱 오차 함수로 설정하고, 학습 알고리즘은 sgd 방식으로 함.\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer = sgd,\n",
    "            metrics = [\"accuracy\"])\n",
    "\n",
    "# 학습을 수행함.\n",
    "history = model.fit(x_train, y_train,\n",
    "                batch_size = batch_size,\n",
    "                epochs = epochs)\n",
    "\n",
    "# 학습을 평가함.\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"테스트 손실값 : {score[0]}\")\n",
    "print(f\"테스트 정확도 : {score[1]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
